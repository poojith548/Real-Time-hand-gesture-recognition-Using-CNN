<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.12.0/dist/tf.min.js" crossorigin="anonymous"></script>
  <!-- <script type="text/javascript" src="https://raw.githubusercontent.com/HarshaSomaraju/gesture-recognition/main/classnames.json"></script> -->
  <!-- <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-node@3.12.0/dist/index.min.js" crossorigin="anonymous"></script> -->
<style>
  #output{
    position: absolute;
    right: 60px;
    top: 50%;
  }

  /* .container {
    position: fixed;
    top: 0;
    left: 0;
    border: 12px solid #000;
    width: 100%;
    height: 100%;
    margin: 0;
    padding: 0;
    box-sizing: border-box;
  } */

  .container-main {
    border: 12px solid #000;
  }

  .container-canvas {
      /* This could be done in one single declaration. See the extended sample. */
      margin-right: auto;
      margin-left: auto;
  }


</style>
</head>

<body>
  <!-- <div class="container">
    <video class="input_video"></video> 
    <canvas class="output_canvas" width="1280px" height="720px">  <h2>Stop</h2></canvas>
  </div>
  <h2 id='output'></h2> -->

  <div class="container container-main">
    <div class="container-canvas">
      <video class="input_video"></video> 
      <canvas class="output_canvas" width="1280px" height="720px"></canvas>
    </div>  
  </div>
  <h2 id='output'></h2>

  <script type="module">
    const model = await tf.loadLayersModel("https://raw.githubusercontent.com/HarshaSomaraju/gesture-recognition/main/model/model.json");
    const labels = ['okay', 'peace', 'thumbs up', 'thumbs down', 'call me', 'stop', 'rock', 'live long', 'fist', 'smile'];
    const outputElement = document.getElementById('output');
    const videoElement = document.getElementsByClassName('input_video')[0];
    videoElement.style.display = 'none';
    const canvasElement = document.getElementsByClassName('output_canvas')[0];
    const canvasCtx = canvasElement.getContext('2d');
    
    function onResults(results) {
      canvasCtx.save();
      canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
      canvasCtx.drawImage(
          results.image, 0, 0, canvasElement.width, canvasElement.height);
      if (results.multiHandLandmarks) {
        for (const landmarks of results.multiHandLandmarks) {
          var lds = [[]];
          for(const lm of landmarks) {
            var lmx = lm.x * 1280;
            var lmy = lm.y * 720;
            lds[0].push([lmx, lmy]);
          }
          var tflds = tf.tensor(lds);
          // console.log(tflds);
          const prediction = model.predict(tflds);
          // prediction.as1D().argMax().print();
          const pred = prediction.as1D().argMax().dataSync()[0];
          // console.log('Prediction is: ');
          // console.log(labels[pred]);
          outputElement.innerText = labels[pred];
          drawConnectors(canvasCtx, landmarks, HAND_CONNECTIONS,
                         {color: '#00FF00', lineWidth: 5});
          drawLandmarks(canvasCtx, landmarks, {color: '#FF0000', lineWidth: 2});
        }
      }
      else{
        outputElement.innerText = '';
      }
      canvasCtx.restore();
    }
    
    const hands = new Hands({locateFile: (file) => {
      return `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`;
    }});
    hands.setOptions({
      maxNumHands: 2,
      modelComplexity: 1,
      minDetectionConfidence: 0.5,
      minTrackingConfidence: 0.5
    });
    hands.onResults(onResults);
    var a;
    
    const camera = new Camera(videoElement, {
      onFrame: async () => {
        await hands.send({image: videoElement});
        // const handler = tfnode.io.fileSystem('model.json');
        // const model = await tf.loadLayersModel("https://raw.githubusercontent.com/HarshaSomaraju/gesture-recognition/main/model/model.json");
        // const example = tf.browser.fromPixels(videoElement);
        // const prediction = model.predict(example);
        // console.log('Prediction is: ');
        // console.log(prediction);
      },
      width: 1280,
      height: 720
    });
    camera.start();
    
  </script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p" crossorigin="anonymous"></script>
</body>
</html>